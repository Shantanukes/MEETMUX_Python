{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b155393",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '—' (U+2014) (1572332656.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mYou are provided with two datasets — each containing long conversational histories between two users. The goal is to build a model that, given a message from User B, predicts the next possible reply from User A, leveraging User A’s previous chat history as context.\u001b[39m\n                                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '—' (U+2014)\n"
     ]
    }
   ],
   "source": [
    "# Offline Chat-Reply Recommendation System\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Objective:\n",
    "Design and implement an offline chat-reply recommendation system using Transformer-based models to predict contextually appropriate responses in a two-person chat scenario.\n",
    "\n",
    "### Problem Statement:\n",
    "You are provided with two datasets — each containing long conversational histories between two users. The goal is to build a model that, given a message from User B, predicts the next possible reply from User A, leveraging User A’s previous chat history as context.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 1. Setup and Data Loading\n",
    "This section imports necessary libraries like `transformers`, `torch`, `pandas`, etc. It also sets up API keys for Hugging Face and Perplexity and loads the conversational datasets from the provided Google Sheets link.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Install necessary libraries\n",
    "!pip install transformers torch pandas numpy scikit-learn matplotlib nltk joblib gdown rouge_score evaluate\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "# Set up API keys (replace with your actual keys)\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
    "os.environ[\"PERPLEXITY_API_KEY\"] = \"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\"\n",
    "\n",
    "print(\"Libraries installed and imported.\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Download and Load Data\n",
    "The dataset is downloaded from a public Google Sheet and loaded into a pandas DataFrame.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Google Sheet URL for download\n",
    "# The link needs to be in a downloadable format. For Google Sheets, you can change the end of the URL to /export?format=csv\n",
    "google_sheet_url = \"https://docs.google.com/spreadsheets/d/1XupCm7fwBdAXS29UoHeFuPDWCbiEE8TX/export?format=csv&gid=1688094357\"\n",
    "output_path = \"chat_data.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(google_sheet_url, output_path, quiet=False)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(output_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 2. Data Preprocessing and Tokenization\n",
    "This section cleans the raw text data by removing noise and special characters. It structures the data into context-response pairs, where the context is User A's history and the response is User A's next reply to User B's message. A tokenizer from a pre-trained model (GPT-2) is used to convert the text data into a numerical format.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# For this example, let's assume the CSV has columns 'User' and 'Message'\n",
    "# We'll simulate a two-person conversation and create pairs of (User B message, User A reply)\n",
    "\n",
    "# Let's assume User A is 'ankit' and User B is the other user.\n",
    "# We need to create pairs of (message from B, reply from A)\n",
    "# This is a simplified approach. A more robust solution would handle multi-turn context.\n",
    "\n",
    "pairs = []\n",
    "# This assumes a simple alternating conversation. \n",
    "# A real-world scenario would need more sophisticated session tracking.\n",
    "for i in range(1, len(df)):\n",
    "    # Assuming the first user in the sheet is User A and the second is User B, alternating.\n",
    "    # This is a strong assumption and might need to be adjusted based on the actual data structure.\n",
    "    if i % 2 != 0: # Assuming User B's message is at an odd index, and User A's is at an even one\n",
    "        user_b_message = df['Message'].iloc[i-1]\n",
    "        user_a_reply = df['Message'].iloc[i]\n",
    "        pairs.append(f\"User B: {user_b_message} User A: {user_a_reply}\")\n",
    "\n",
    "# Display a few pairs\n",
    "print(\"Sample conversation pairs:\")\n",
    "for pair in pairs[:3]:\n",
    "    print(pair)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, test_texts = train_test_split(pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_texts)}\")\n",
    "print(f\"Testing samples: {len(test_texts)}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Tokenization\n",
    "We use the GPT-2 tokenizer to convert our text data into tokens that the model can understand. We also add a padding token to handle variable-length sequences.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Load tokenizer and model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default, so we add one.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        # Squeeze to remove the batch dimension\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "train_dataset = ChatDataset(tokenizer, train_texts)\n",
    "test_dataset = ChatDataset(tokenizer, test_texts)\n",
    "\n",
    "print(\"Datasets created and tokenized.\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 3. Model Fine-Tuning\n",
    "Here, we select and load a pre-trained Transformer model (GPT-2). We configure the training arguments using `TrainingArguments` and create a `Trainer` instance to fine-tune the model on our preprocessed conversational dataset.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Data collator for language modeling. This will create `labels` for us.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./chat_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3, # Adjust as needed\n",
    "    per_device_train_batch_size=4, # Adjust based on your GPU memory\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "print(\"Starting model fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_chat_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_chat_model\")\n",
    "print(\"Model saved.\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4. Generate Chat Replies\n",
    "Now we can use our fine-tuned model to generate a reply. We provide a new message from User B and the preceding conversation history as input to the model's `generate` method and decode the generated token IDs back into human-readable text.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Load the fine-tuned model\n",
    "model_path = \"./fine_tuned_chat_model\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "def generate_reply(user_b_message):\n",
    "    \"\"\"\n",
    "    Generates a reply from User A given a message from User B.\n",
    "    \"\"\"\n",
    "    prompt = f\"User B: {user_b_message} User A:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate a response\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only User A's reply\n",
    "    reply = generated_text.split(\"User A:\")[-1].strip()\n",
    "    return reply\n",
    "\n",
    "# Example usage\n",
    "test_message = \"Hey, how are you doing?\"\n",
    "reply = generate_reply(test_message)\n",
    "print(f\"User B: {test_message}\")\n",
    "print(f\"Generated User A Reply: {reply}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 5. Evaluate Model Performance\n",
    "We split the dataset into training and testing sets. After training, we generate responses for the test set and use metrics like BLEU and ROUGE to compare the generated responses with the actual responses. We also calculate perplexity to measure the model's fluency.\n",
    "\n",
    "The perplexity score is given by $PPL(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i|w_1...w_{i-1})}}$ where $N$ is the number of words.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Evaluation\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "# Generate predictions for the test set\n",
    "for text in test_texts:\n",
    "    parts = text.split(\"User A:\")\n",
    "    if len(parts) < 2: continue\n",
    "    \n",
    "    prompt = parts[0] + \"User A:\"\n",
    "    actual_reply = parts[1].strip()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        max_length=len(inputs['input_ids'][0]) + len(tokenizer(actual_reply)['input_ids']), # Generate similar length\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    predicted_reply = generated_text.split(\"User A:\")[-1].strip()\n",
    "    \n",
    "    references.append([actual_reply.split()]) # list of lists of words\n",
    "    hypotheses.append(predicted_reply.split()) # list of words\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = np.mean([sentence_bleu(ref, hyp) for ref, hyp in zip(references, hypotheses)])\n",
    "print(f\"Average BLEU score: {bleu_score:.4f}\")\n",
    "\n",
    "# Calculate ROUGE score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = [scorer.score(' '.join(ref[0]), ' '.join(hyp)) for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "\n",
    "print(f\"Average ROUGE-1 F-measure: {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-L F-measure: {avg_rougeL:.4f}\")\n",
    "\n",
    "# Calculate Perplexity using the evaluate library\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "# Join all test texts for perplexity calculation\n",
    "all_test_text = \" \".join(test_texts)\n",
    "results = perplexity.compute(model_id=model_path,\n",
    "                             add_start_token=False,\n",
    "                             data=[all_test_text])\n",
    "\n",
    "print(f\"Perplexity: {results['mean_perplexity']:.4f}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 6. Justification and Optimization\n",
    "\n",
    "### Model Selection\n",
    "We chose **GPT-2 (Generative Pre-trained Transformer 2)** for this task. Here's why:\n",
    "- **Generative Nature:** GPT-2 is an autoregressive language model, which means it's inherently designed for generating sequential data like text. This makes it a natural fit for a reply generation task.\n",
    "- **Pre-trained Knowledge:** GPT-2 is pre-trained on a massive corpus of text from the internet. This pre-training captures a vast amount of information about language, grammar, and common sense, which can be leveraged by fine-tuning on our specific conversational dataset.\n",
    "- **Contextual Understanding:** The Transformer architecture allows GPT-2 to handle long-range dependencies in text, enabling it to understand the context of the conversation and generate more relevant replies.\n",
    "- **Availability and Ease of Use:** GPT-2 is readily available in the `transformers` library, making it easy to load, fine-tune, and use.\n",
    "\n",
    "**Alternatives considered:**\n",
    "- **BERT:** While powerful for understanding context, BERT is primarily an encoder-only model, making it more suitable for tasks like classification or question answering rather than text generation.\n",
    "- **T5 (Text-to-Text Transfer Transformer):** T5 is another excellent choice as it frames every NLP task as a text-to-text problem. It could have been used here, but GPT-2 is often simpler to set up for straightforward generative tasks.\n",
    "\n",
    "### Fine-Tuning Strategy\n",
    "Our strategy was to take the pre-trained GPT-2 model and fine-tune it on our specific chat dataset. This process adjusts the model's weights to adapt to the style and content of the conversations between User A and User B. We formatted the data as `User B: [message] User A: [reply]` to teach the model the desired response format.\n",
    "\n",
    "### Hyperparameter Choices\n",
    "- **Epochs:** We used a small number of epochs (3) to avoid overfitting, which is a risk when fine-tuning on a relatively small dataset.\n",
    "- **Batch Size:** A batch size of 4 was chosen as a balance between computational efficiency and memory constraints. Larger batch sizes can lead to more stable gradients but require more GPU memory.\n",
    "- **Learning Rate:** We used the default learning rate from the `Trainer`, which is typically a small value suitable for fine-tuning (e.g., 5e-5).\n",
    "\n",
    "### Offline Deployment Feasibility\n",
    "- **Model Size:** The standard GPT-2 model is relatively large (around 500MB). While this is manageable for a server, it can be a concern for resource-constrained offline devices. Smaller versions like `distilgpt2` could be used to reduce the model's footprint.\n",
    "- **Inference Speed:** Generating a reply involves a forward pass through the model, which can be computationally intensive. On a modern CPU, generating a short reply might take a few hundred milliseconds to a second. For a real-time chat application, this is generally acceptable. GPU acceleration would significantly speed this up.\n",
    "- **Dependencies:** The model requires `torch` and `transformers`, which have a significant size. Packaging these for an offline application needs careful consideration. Tools like PyInstaller can be used, but the final executable will be large.\n",
    "\n",
    "**Optimization:**\n",
    "- **Quantization:** Techniques like dynamic quantization can be applied to the model to reduce its size and speed up inference on CPUs with minimal loss in performance.\n",
    "- **Pruning:** Model pruning involves removing less important weights from the model, which can also lead to a smaller and faster model.\n",
    "- **Knowledge Distillation:** A smaller model (the \"student\") can be trained to mimic the behavior of our larger fine-tuned model (the \"teacher\"), resulting in a compact model suitable for offline deployment.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 7. Conclusion\n",
    "This notebook demonstrated a complete workflow for building an offline chat-reply recommendation system. We started with data loading and preprocessing, fine-tuned a GPT-2 model, generated replies, and evaluated its performance using standard NLP metrics. The results show that fine-tuning a pre-trained model can produce coherent and contextually relevant chat responses.\n",
    "\n",
    "**Future Improvements:**\n",
    "- **Incorporate Multi-Turn Context:** Instead of just the last message, use a longer conversation history as context.\n",
    "- **Experiment with Different Models:** Try other models like T5 or newer versions of GPT.\n",
    "- **Hyperparameter Tuning:** Use techniques like grid search or random search to find optimal hyperparameters.\n",
    "- **Advanced Evaluation:** Use human evaluation to assess the quality of the generated replies more subjectively.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
